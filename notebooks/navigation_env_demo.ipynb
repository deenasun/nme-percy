{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Navigation Environment Demonstration\n",
    "\n",
    "This notebook demonstrates how to use the Navigation Environment to create and test reinforcement learning agents for navigating through grid environments derived from depth maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Add the project root to the Python path\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Necessary Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'src'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Import environment and visualization modules\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01menvironments\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnavigation_env\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NavigationEnv\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mvisualization\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01menv_visualizer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      4\u001b[39m     visualize_path, visualize_trajectory, \n\u001b[32m      5\u001b[39m     create_trajectory_animation, display_env_info\n\u001b[32m      6\u001b[39m )\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdepth_estimation\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmidas\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m estimate_depth\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'src'"
     ]
    }
   ],
   "source": [
    "# Import environment and visualization modules\n",
    "from src.environments.navigation_env import NavigationEnv\n",
    "from src.visualization.env_visualizer import (\n",
    "    visualize_path, visualize_trajectory, \n",
    "    create_trajectory_animation, display_env_info\n",
    ")\n",
    "from src.depth_estimation.midas import estimate_depth\n",
    "from src.grid_conversion.converter import depth_to_grid, add_start_goal_points\n",
    "from src.visualization.grid_visualizer import visualize_depth_to_grid_comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generate a Grid from an RGB Image\n",
    "\n",
    "We'll start by generating a depth map from an RGB image, and then convert it to a navigable grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to an RGB image\n",
    "image_path = \"../data/images/sample.jpg\"  # Replace with your image path\n",
    "\n",
    "# Check if image exists, otherwise use a placeholder\n",
    "if not os.path.exists(image_path):\n",
    "    print(f\"Image {image_path} not found. Please download a sample image first.\")\n",
    "    # You can download a sample image using the provided script\n",
    "    print(\"You can run: python scripts/download_sample_image.py\")\n",
    "    # For now, we'll create a simple grid manually\n",
    "    grid = np.zeros((32, 32), dtype=np.int8)\n",
    "    # Add walls around the edges\n",
    "    grid[0, :] = 1\n",
    "    grid[-1, :] = 1\n",
    "    grid[:, 0] = 1\n",
    "    grid[:, -1] = 1\n",
    "    # Add some obstacles\n",
    "    grid[10:20, 10:20] = 1\n",
    "    grid[5:10, 15:25] = 1\n",
    "    # Ensure start and goal positions are free\n",
    "    grid[1:3, 1:3] = 0\n",
    "    grid[-3:-1, -3:-1] = 0\n",
    "    \n",
    "    # Create dummy RGB and depth for visualization\n",
    "    rgb_image = np.zeros((100, 100, 3), dtype=np.uint8)\n",
    "    depth_map = np.zeros((100, 100), dtype=np.float32)\n",
    "else:\n",
    "    # Generate depth map\n",
    "    rgb_image, depth_map = estimate_depth(image_path)\n",
    "    \n",
    "    # Convert to grid\n",
    "    grid_size = 32\n",
    "    threshold = 0.6\n",
    "    grid = depth_to_grid(\n",
    "        depth_map=depth_map,\n",
    "        grid_size=grid_size,\n",
    "        threshold_factor=threshold,\n",
    "        smoothing=True,\n",
    "        kernel_size=3\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the conversion from RGB to depth to grid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add start and goal points to the grid\n",
    "start_pos = (0, 0)\n",
    "goal_pos = (grid.shape[0] - 1, grid.shape[1] - 1)\n",
    "grid_with_points = add_start_goal_points(grid, start_pos, goal_pos)\n",
    "\n",
    "# Visualize the transformation\n",
    "visualize_depth_to_grid_comparison(rgb_image, depth_map, grid_with_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create a Navigation Environment\n",
    "\n",
    "Now that we have a grid, let's create a navigation environment and explore it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the environment\n",
    "env = NavigationEnv(\n",
    "    grid=grid,\n",
    "    start_pos=start_pos,\n",
    "    goal_pos=goal_pos,\n",
    "    max_steps=200,\n",
    "    render_mode=None  # We'll use our custom visualization\n",
    ")\n",
    "\n",
    "# Display environment information\n",
    "display_env_info(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run a Random Agent\n",
    "\n",
    "Let's run a random agent in the environment to see how it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the environment\n",
    "observation, info = env.reset()\n",
    "\n",
    "# Initialize variables\n",
    "done = False\n",
    "truncated = False\n",
    "total_reward = 0\n",
    "step_count = 0\n",
    "max_steps = 100\n",
    "trajectory = [env.agent_pos]  # Start with initial position\n",
    "rewards = []\n",
    "\n",
    "# Run the agent\n",
    "while not (done or truncated) and step_count < max_steps:\n",
    "    # Take a random action\n",
    "    action = env.action_space.sample()\n",
    "    \n",
    "    # Step in the environment\n",
    "    observation, reward, done, truncated, info = env.step(action)\n",
    "    \n",
    "    # Update tracking variables\n",
    "    total_reward += reward\n",
    "    step_count += 1\n",
    "    trajectory.append(env.agent_pos)\n",
    "    rewards.append(reward)\n",
    "    \n",
    "# Print results\n",
    "print(f\"Simulation complete after {step_count} steps\")\n",
    "print(f\"Total reward: {total_reward:.1f}\")\n",
    "print(f\"Final position: {env.agent_pos}\")\n",
    "if env.agent_pos == env.goal_pos:\n",
    "    print(\"Goal reached!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualize the Trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the trajectory\n",
    "visualize_trajectory(\n",
    "    env=env,\n",
    "    trajectory=trajectory,\n",
    "    rewards=rewards,\n",
    "    title=f\"Random Agent Trajectory (Reward: {total_reward:.1f})\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Create an Animation of the Trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an animation\n",
    "create_trajectory_animation(\n",
    "    env=env,\n",
    "    trajectory=trajectory,\n",
    "    title=f\"Random Agent (Reward: {total_reward:.1f})\",\n",
    "    interval=200  # milliseconds between frames\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Implement a Simple Heuristic Policy\n",
    "\n",
    "Let's create a simple heuristic policy that tries to move toward the goal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heuristic_policy(observation):\n",
    "    \"\"\"A simple heuristic policy that tries to move toward the goal.\n",
    "    \n",
    "    Args:\n",
    "        observation: The observation from the environment.\n",
    "        \n",
    "    Returns:\n",
    "        The action to take (0: up, 1: right, 2: down, 3: left).\n",
    "    \"\"\"\n",
    "    # Get agent and goal positions\n",
    "    agent_pos = tuple(observation['position'])\n",
    "    goal_pos = tuple(observation['goal'])\n",
    "    grid = observation['grid']\n",
    "    \n",
    "    # Determine direction to goal\n",
    "    row_diff = goal_pos[0] - agent_pos[0]\n",
    "    col_diff = goal_pos[1] - agent_pos[1]\n",
    "    \n",
    "    # Prioritize moving in the direction with larger difference\n",
    "    if abs(row_diff) > abs(col_diff):\n",
    "        # Move vertically (up or down)\n",
    "        if row_diff < 0:\n",
    "            # Goal is above, try to move up\n",
    "            action = 0  # up\n",
    "        else:\n",
    "            # Goal is below, try to move down\n",
    "            action = 2  # down\n",
    "    else:\n",
    "        # Move horizontally (left or right)\n",
    "        if col_diff > 0:\n",
    "            # Goal is to the right, try to move right\n",
    "            action = 1  # right\n",
    "        else:\n",
    "            # Goal is to the left, try to move left\n",
    "            action = 3  # left\n",
    "    \n",
    "    # Check if the action would lead to an obstacle and avoid it\n",
    "    row, col = agent_pos\n",
    "    if action == 0 and row > 0 and grid[row-1, col] == 1:  # Up leads to obstacle\n",
    "        # Try other directions instead\n",
    "        if col < grid.shape[1]-1 and grid[row, col+1] == 0:  # Right is free\n",
    "            action = 1\n",
    "        elif row < grid.shape[0]-1 and grid[row+1, col] == 0:  # Down is free\n",
    "            action = 2\n",
    "        elif col > 0 and grid[row, col-1] == 0:  # Left is free\n",
    "            action = 3\n",
    "    elif action == 1 and col < grid.shape[1]-1 and grid[row, col+1] == 1:  # Right leads to obstacle\n",
    "        # Try other directions\n",
    "        if row > 0 and grid[row-1, col] == 0:  # Up is free\n",
    "            action = 0\n",
    "        elif row < grid.shape[0]-1 and grid[row+1, col] == 0:  # Down is free\n",
    "            action = 2\n",
    "        elif col > 0 and grid[row, col-1] == 0:  # Left is free\n",
    "            action = 3\n",
    "    elif action == 2 and row < grid.shape[0]-1 and grid[row+1, col] == 1:  # Down leads to obstacle\n",
    "        # Try other directions\n",
    "        if col < grid.shape[1]-1 and grid[row, col+1] == 0:  # Right is free\n",
    "            action = 1\n",
    "        elif row > 0 and grid[row-1, col] == 0:  # Up is free\n",
    "            action = 0\n",
    "        elif col > 0 and grid[row, col-1] == 0:  # Left is free\n",
    "            action = 3\n",
    "    elif action == 3 and col > 0 and grid[row, col-1] == 1:  # Left leads to obstacle\n",
    "        # Try other directions\n",
    "        if row > 0 and grid[row-1, col] == 0:  # Up is free\n",
    "            action = 0\n",
    "        elif col < grid.shape[1]-1 and grid[row, col+1] == 0:  # Right is free\n",
    "            action = 1\n",
    "        elif row < grid.shape[0]-1 and grid[row+1, col] == 0:  # Down is free\n",
    "            action = 2\n",
    "    \n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the heuristic policy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the environment\n",
    "observation, info = env.reset()\n",
    "\n",
    "# Initialize variables\n",
    "done = False\n",
    "truncated = False\n",
    "total_reward = 0\n",
    "step_count = 0\n",
    "max_steps = 100\n",
    "trajectory = [env.agent_pos]  # Start with initial position\n",
    "rewards = []\n",
    "\n",
    "# Run the agent\n",
    "while not (done or truncated) and step_count < max_steps:\n",
    "    # Get action from heuristic policy\n",
    "    action = heuristic_policy(observation)\n",
    "    \n",
    "    # Step in the environment\n",
    "    observation, reward, done, truncated, info = env.step(action)\n",
    "    \n",
    "    # Update tracking variables\n",
    "    total_reward += reward\n",
    "    step_count += 1\n",
    "    trajectory.append(env.agent_pos)\n",
    "    rewards.append(reward)\n",
    "    \n",
    "# Print results\n",
    "print(f\"Simulation complete after {step_count} steps\")\n",
    "print(f\"Total reward: {total_reward:.1f}\")\n",
    "print(f\"Final position: {env.agent_pos}\")\n",
    "if env.agent_pos == env.goal_pos:\n",
    "    print(\"Goal reached!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the heuristic policy's trajectory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the trajectory\n",
    "visualize_trajectory(\n",
    "    env=env,\n",
    "    trajectory=trajectory,\n",
    "    rewards=rewards,\n",
    "    title=f\"Heuristic Agent Trajectory (Reward: {total_reward:.1f})\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an animation of the heuristic policy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an animation\n",
    "create_trajectory_animation(\n",
    "    env=env,\n",
    "    trajectory=trajectory,\n",
    "    title=f\"Heuristic Agent (Reward: {total_reward:.1f})\",\n",
    "    interval=200  # milliseconds between frames\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Next Steps\n",
    "\n",
    "In the next phase of the project, we'll implement reinforcement learning algorithms to learn optimal policies for navigating these environments. Some potential algorithms include:\n",
    "\n",
    "1. Q-Learning\n",
    "2. Deep Q-Networks (DQN)\n",
    "3. Proximal Policy Optimization (PPO)\n",
    "4. A* and other path planning algorithms"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
